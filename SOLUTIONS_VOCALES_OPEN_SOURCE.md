# üéôÔ∏è SOLUTIONS VOCALES OPEN SOURCE - IAcompagnon

## üéØ OBJECTIF

Impl√©menter une voix **naturelle, apaisante, gratuite** pour l'avatar th√©rapeutique.

**Crit√®res** :
- ‚úÖ Open source (gratuit)
- ‚úÖ Qualit√© proche d'ElevenLabs
- ‚úÖ Voix fran√ßaise naturelle
- ‚úÖ Latence acceptable (<2s)
- ‚úÖ Self-hosted ou API gratuite

---

## üèÜ MEILLEURES SOLUTIONS OPEN SOURCE 2025

### 1. ü•á COQUI TTS (XTTS-v2) ‚≠ê RECOMMAND√â

**Description** : Le meilleur TTS open source actuel, qualit√© proche ElevenLabs

**Qualit√©** : 9/10 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Facilit√©** : 7/10  
**Co√ªt** : GRATUIT  

**Avantages** :
- ‚úÖ Voix **ultra r√©alistes** (deep learning avanc√©)
- ‚úÖ Support fran√ßais natif excellent
- ‚úÖ Voice cloning possible (11 secondes d'audio suffisent)
- ‚úÖ Multi-locuteurs (choix de voix)
- ‚úÖ √âmotions dans la voix
- ‚úÖ Latence ~1-2s par phrase

**Inconv√©nients** :
- ‚ö†Ô∏è N√©cessite GPU (ou CPU puissant)
- ‚ö†Ô∏è Installation un peu technique
- ‚ö†Ô∏è 2-3 GB VRAM minimum

**Voix fran√ßaises incluses** :
- `tts_models/fr/css10/vits` - Voix f√©minine claire
- `tts_models/multilingual/multi-dataset/xtts_v2` - Multi-langues (le meilleur)

**Installation** :

```bash
# Avec pip
pip install TTS

# Avec Docker (recommand√©)
docker pull ghcr.io/coqui-ai/tts

# Lancer serveur TTS
docker run -it -p 5002:5002 ghcr.io/coqui-ai/tts --model_name tts_models/multilingual/multi-dataset/xtts_v2
```

**API Endpoint** :

```python
# backend/api-gateway/app/routes/voice.py

from TTS.api import TTS
import base64
import io

# Init TTS (au d√©marrage serveur)
tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False, gpu=True)

@router.post("/api/voice/synthesize")
async def synthesize_coqui(text: str, voice: str = "default"):
    """
    G√©n√®re audio avec Coqui TTS
    """
    # Fichier temporaire
    temp_file = "/tmp/output.wav"
    
    # G√©n√©rer audio
    tts.tts_to_file(
        text=text,
        file_path=temp_file,
        language="fr",
        speaker_wav=None  # Ou path vers voix custom
    )
    
    # Lire et encoder en base64
    with open(temp_file, "rb") as f:
        audio_bytes = f.read()
        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
    
    return {
        "audio": audio_base64,
        "format": "wav",
        "model": "coqui-xtts-v2"
    }
```

**Voice Cloning (optionnel)** :

Si vous voulez une voix **sp√©cifique** pour Hel≈ç :

```python
# 1. Enregistrer 10-15 secondes de voix souhait√©e
speaker_wav = "/path/to/voice_sample.wav"

# 2. Utiliser cette voix
tts.tts_to_file(
    text=text,
    file_path=temp_file,
    language="fr",
    speaker_wav=speaker_wav  # Clone cette voix !
)
```

**D√©mo en ligne** : https://huggingface.co/spaces/coqui/xtts

---

### 2. ü•à PIPER TTS ‚ö° Plus rapide

**Description** : TTS ultra rapide, qualit√© tr√®s correcte

**Qualit√©** : 7.5/10  
**Facilit√©** : 9/10  
**Co√ªt** : GRATUIT  

**Avantages** :
- ‚úÖ **Tr√®s rapide** (<500ms par phrase)
- ‚úÖ Fonctionne sur CPU (pas besoin GPU)
- ‚úÖ Petit (~50MB par voix)
- ‚úÖ Installation simple
- ‚úÖ Support fran√ßais

**Inconv√©nients** :
- ‚ö†Ô∏è Qualit√© l√©g√®rement inf√©rieure √† Coqui
- ‚ö†Ô∏è Moins naturel pour longues phrases

**Voix fran√ßaises disponibles** :
- `fr_FR-siwis-medium` - Voix f√©minine douce
- `fr_FR-upmc-medium` - Voix f√©minine claire

**Installation** :

```bash
# Installer Piper
pip install piper-tts

# T√©l√©charger voix fran√ßaise
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-siwis-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-siwis-medium.onnx.json

# Utiliser
echo "Bonjour, je suis Hel≈ç" | piper --model fr_FR-siwis-medium.onnx --output_file output.wav
```

**API Endpoint** :

```python
import subprocess
import base64

@router.post("/api/voice/synthesize")
async def synthesize_piper(text: str):
    """
    G√©n√®re audio avec Piper TTS
    """
    temp_file = "/tmp/output.wav"
    
    # Appel Piper
    subprocess.run([
        "piper",
        "--model", "/path/to/fr_FR-siwis-medium.onnx",
        "--output_file", temp_file
    ], input=text.encode(), check=True)
    
    # Lire et encoder
    with open(temp_file, "rb") as f:
        audio_bytes = f.read()
        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
    
    return {
        "audio": audio_base64,
        "format": "wav",
        "model": "piper"
    }
```

**D√©mo** : https://rhasspy.github.io/piper-samples/

---

### 3. ü•â MOZILLA TTS / COQUI-AI (Ancienne version)

**Description** : Pr√©d√©cesseur de Coqui TTS, toujours valable

**Qualit√©** : 7/10  
**Facilit√©** : 6/10  
**Co√ªt** : GRATUIT  

**Note** : Remplac√© par XTTS-v2, utiliser plut√¥t option 1

---

### 4. üåê STYLETTS 2

**Description** : TTS √©mergent avec style expressif

**Qualit√©** : 8.5/10  
**Facilit√©** : 5/10  
**Co√ªt** : GRATUIT  

**Avantages** :
- ‚úÖ Tr√®s naturel
- ‚úÖ Contr√¥le √©motions
- ‚úÖ Qualit√© excellente

**Inconv√©nients** :
- ‚ö†Ô∏è Moins mature
- ‚ö†Ô∏è Installation complexe
- ‚ö†Ô∏è Support fran√ßais limit√©

**GitHub** : https://github.com/yl4579/StyleTTS2

---

## üìä COMPARATIF D√âTAILL√â

| Solution | Qualit√© voix | Vitesse | GPU requis | Fran√ßais | Facilit√© | Recommandation |
|----------|--------------|---------|------------|----------|----------|----------------|
| **Coqui XTTS-v2** | 9/10 ‚≠ê‚≠ê‚≠ê | 1-2s | Oui (ou CPU lent) | Excellent | Moyenne | **BETA** |
| **Piper TTS** | 7.5/10 | <500ms ‚ö° | Non | Bon | Facile | MVP rapide |
| **StyleTTS 2** | 8.5/10 | 2-3s | Oui | Limit√© | Difficile | Future |
| **Web Speech API** | 7/10 | Temps r√©el | Non | Bon | Tr√®s facile | Backup |

---

## üéØ RECOMMANDATION POUR IACOMPAGNON

### PHASE 1 : MVP (Semaine 1) - PIPER TTS ‚ö°

**Pourquoi Piper d'abord** :
- ‚úÖ Installation en 10 minutes
- ‚úÖ Fonctionne sur n'importe quel serveur
- ‚úÖ Latence ultra faible (<500ms)
- ‚úÖ Qualit√© correcte (7.5/10)
- ‚úÖ Pas besoin GPU (√©conomies h√©bergement)

**Code complet** :

```python
# backend/api-gateway/app/services/tts.py

import subprocess
import tempfile
import base64
import os

class PiperTTS:
    def __init__(self, model_path="/app/models/fr_FR-siwis-medium.onnx"):
        self.model_path = model_path
        
        # V√©rifier mod√®le existe
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")
    
    def synthesize(self, text: str) -> bytes:
        """
        G√©n√®re audio depuis texte
        Retourne bytes WAV
        """
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            temp_file = tmp.name
        
        try:
            # Appel Piper
            process = subprocess.run(
                [
                    "piper",
                    "--model", self.model_path,
                    "--output_file", temp_file
                ],
                input=text.encode('utf-8'),
                capture_output=True,
                check=True
            )
            
            # Lire r√©sultat
            with open(temp_file, "rb") as f:
                audio_bytes = f.read()
            
            return audio_bytes
            
        finally:
            # Nettoyer
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    def synthesize_base64(self, text: str) -> str:
        """
        Retourne audio en base64 pour frontend
        """
        audio_bytes = self.synthesize(text)
        return base64.b64encode(audio_bytes).decode('utf-8')

# Init global
piper_tts = PiperTTS()
```

**Endpoint API** :

```python
# backend/api-gateway/app/routes/voice.py

from fastapi import APIRouter
from app.services.tts import piper_tts

router = APIRouter()

@router.post("/api/voice/synthesize")
async def synthesize_speech(text: str):
    """
    Synth√®se vocale avec Piper TTS
    """
    try:
        audio_base64 = piper_tts.synthesize_base64(text)
        
        return {
            "audio": audio_base64,
            "format": "wav",
            "duration_ms": estimate_duration(text),  # ~150 mots/min
            "model": "piper-fr-siwis"
        }
    except Exception as e:
        return {
            "error": str(e),
            "fallback": "web_speech_api"  # Frontend peut fallback
        }

def estimate_duration(text: str) -> int:
    """Estime dur√©e audio en ms"""
    words = len(text.split())
    # ~150 mots/minute = 2.5 mots/sec = 400ms/mot
    return words * 400
```

**Docker** :

```dockerfile
# Dockerfile

FROM python:3.11-slim

# Installer Piper
RUN pip install piper-tts

# T√©l√©charger mod√®le fran√ßais
WORKDIR /app/models
RUN wget https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-siwis-medium.onnx
RUN wget https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-siwis-medium.onnx.json

WORKDIR /app
COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Taille image Docker** : ~300MB (l√©ger !)

---

### PHASE 2 : BETA (Mois 2) - COQUI XTTS-v2 üöÄ

**Apr√®s validation MVP**, upgrader vers Coqui pour qualit√© premium.

**Pourquoi attendre** :
- N√©cessite GPU ‚Üí Co√ªt h√©bergement plus √©lev√©
- Installation plus complexe
- Mais qualit√© 9/10 (vs 7.5/10 Piper)

**H√©bergement avec GPU** :

**Option A : RunPod (Recommand√©)**
- GPU RTX 3060 : ~0.30$/heure = ~220‚Ç¨/mois si 24/7
- Ou on-demand : Payer que quand utilis√©
- Autoscaling possible

**Option B : Hetzner Cloud GPU**
- GPU instances : √Ä partir 1‚Ç¨/heure
- Ou CPU puissant : CCX33 √† 63‚Ç¨/mois (fonctionne mais lent)

**Code Coqui** :

```python
# backend/api-gateway/app/services/tts.py

from TTS.api import TTS
import torch

class CoquiTTS:
    def __init__(self):
        # Charger mod√®le XTTS-v2
        self.tts = TTS(
            model_name="tts_models/multilingual/multi-dataset/xtts_v2",
            progress_bar=False,
            gpu=torch.cuda.is_available()
        )
        
        # Voix par d√©faut (optionnel : voice cloning)
        self.speaker_wav = None
    
    def synthesize(self, text: str) -> bytes:
        """
        G√©n√®re audio haute qualit√©
        """
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            temp_file = tmp.name
        
        try:
            self.tts.tts_to_file(
                text=text,
                file_path=temp_file,
                language="fr",
                speaker_wav=self.speaker_wav,
                speed=0.95  # L√©g√®rement plus lent = plus apaisant
            )
            
            with open(temp_file, "rb") as f:
                audio_bytes = f.read()
            
            return audio_bytes
            
        finally:
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    def set_voice(self, speaker_wav_path: str):
        """
        Change la voix (voice cloning)
        """
        self.speaker_wav = speaker_wav_path

# Init
coqui_tts = CoquiTTS()
```

---

## üé§ STT (SPEECH-TO-TEXT) - Solutions Open Source

### Option 1 : Whisper (OpenAI) ‚≠ê RECOMMAND√â

**Qualit√©** : 9.5/10  
**Co√ªt** : 0.006$/minute via API OU Gratuit si self-hosted  

**API OpenAI** (Simple) :

```python
import openai

@router.post("/api/voice/transcribe")
async def transcribe_audio(audio: UploadFile):
    """
    Transcrit audio en texte avec Whisper API
    """
    audio_bytes = await audio.read()
    
    # Sauvegarder temporairement
    temp_path = f"/tmp/{audio.filename}"
    with open(temp_path, "wb") as f:
        f.write(audio_bytes)
    
    # Transcription
    with open(temp_path, "rb") as audio_file:
        transcript = openai.Audio.transcribe(
            model="whisper-1",
            file=audio_file,
            language="fr"
        )
    
    os.remove(temp_path)
    
    return {"text": transcript.text}
```

**Co√ªt** : 0.006$/min = ~0.36‚Ç¨/heure de transcription

**Self-hosted Whisper** (Gratuit) :

```python
import whisper

# Charger mod√®le (une fois au d√©marrage)
model = whisper.load_model("medium")  # Ou "large" si GPU puissant

@router.post("/api/voice/transcribe")
async def transcribe_audio(audio: UploadFile):
    """
    Transcrit avec Whisper local
    """
    audio_bytes = await audio.read()
    temp_path = f"/tmp/{audio.filename}"
    
    with open(temp_path, "wb") as f:
        f.write(audio_bytes)
    
    # Transcription
    result = model.transcribe(temp_path, language="fr")
    
    os.remove(temp_path)
    
    return {"text": result["text"]}
```

**Taille mod√®les** :
- `tiny` : 75MB, rapide, qualit√© 6/10
- `base` : 140MB, qualit√© 7/10
- `small` : 460MB, qualit√© 8/10
- `medium` : 1.5GB, qualit√© 9/10 ‚≠ê
- `large` : 3GB, qualit√© 9.5/10

**Recommandation** : `medium` pour bon compromis qualit√©/vitesse

---

### Option 2 : Vosk (Ultra rapide)

**Qualit√©** : 7/10  
**Co√ªt** : GRATUIT  
**Vitesse** : Temps r√©el  

**Avantages** :
- ‚úÖ Fonctionne hors ligne
- ‚úÖ Tr√®s l√©ger
- ‚úÖ Temps r√©el (pas de latence)

**Inconv√©nients** :
- ‚ö†Ô∏è Qualit√© inf√©rieure √† Whisper

```python
from vosk import Model, KaldiRecognizer
import wave

model = Model(lang="fr")

def transcribe_vosk(audio_path: str) -> str:
    wf = wave.open(audio_path, "rb")
    rec = KaldiRecognizer(model, wf.getframerate())
    
    result = ""
    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            result += rec.Result()
    
    return result
```

---

## üí∞ CO√õTS FINAUX PAR SOLUTION

### Solution 1 : Piper + Whisper API (Recommand√© Beta)

```
TTS (Piper) : 0‚Ç¨ (self-hosted)
STT (Whisper API) : 0.006$/min

User moyen : 20 min vocal/mois
Co√ªt : 20 √ó 0.006$ = 0.12$/mois = 0.11‚Ç¨/user/mois

H√©bergement : Hetzner CPX31 (13.90‚Ç¨/mois)
  ‚Üí Peut g√©rer 200 users simultan√©s

CO√õT TOTAL : 0.11‚Ç¨/user/mois
```

### Solution 2 : Coqui + Whisper Self-hosted (Premium)

```
TTS (Coqui) : 0‚Ç¨ (mais n√©cessite GPU)
STT (Whisper local) : 0‚Ç¨

H√©bergement GPU : RunPod RTX 3060 = 220‚Ç¨/mois
  ‚Üí Ou on-demand : 0.30$/heure √ó usage r√©el

Si usage 8h/jour : ~72‚Ç¨/mois
Si 100 users actifs : 0.72‚Ç¨/user/mois

CO√õT TOTAL : 0.72‚Ç¨/user/mois (avec GPU on-demand)
```

### Solution 3 : Web Speech API (Fallback gratuit)

```
TTS : 0‚Ç¨ (navigateur)
STT : 0‚Ç¨ (navigateur)

CO√õT TOTAL : 0‚Ç¨ (mais qualit√© 7/10)
```

---

## üéØ ARCHITECTURE RECOMMAND√âE (Multi-niveaux)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FRONTEND                               ‚îÇ
‚îÇ  1. Web Speech API (fallback gratuit)  ‚îÇ
‚îÇ  2. API backend TTS si disponible      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BACKEND                                ‚îÇ
‚îÇ  Phase 1 : Piper TTS + Whisper API     ‚îÇ
‚îÇ    ‚Üí Co√ªt : 0.11‚Ç¨/user/mois            ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Phase 2 : Coqui XTTS-v2 (si succ√®s)   ‚îÇ
‚îÇ    ‚Üí Co√ªt : 0.72‚Ç¨/user/mois            ‚îÇ
‚îÇ    ‚Üí Qualit√© premium 9/10              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Frontend d√©tecte** :
1. Navigateur supporte Web Speech ? ‚Üí Utiliser
2. Backend TTS disponible ? ‚Üí Utiliser (meilleure qualit√©)
3. Sinon ‚Üí Message "Vocal non disponible"

---

## ‚úÖ PLAN D'IMPL√âMENTATION

### Semaine 1 : STT + TTS Basique

**Jour 1** : Backend STT
- [ ] Installer Whisper API (ou local)
- [ ] Endpoint `/api/voice/transcribe`
- [ ] Tests transcription fran√ßaise

**Jour 2** : Backend TTS
- [ ] Installer Piper TTS
- [ ] T√©l√©charger voix fr_FR-siwis
- [ ] Endpoint `/api/voice/synthesize`
- [ ] Tests g√©n√©ration voix

**Jour 3** : Frontend integration
- [ ] Hook useSpeechRecognition (fallback Web Speech)
- [ ] Hook useBackendVoice (appel API)
- [ ] Bouton micro dans chat
- [ ] Tests enregistrement ‚Üí transcription

**Jour 4** : Overlay vocal
- [ ] Avatar fullscreen mode vocal
- [ ] Lecture audio avatar
- [ ] Animation synchronis√©e
- [ ] Tests conversation compl√®te

**Jour 5** : Polish + Tests
- [ ] Gestion erreurs
- [ ] Feedback visuel utilisateur
- [ ] Tests mobile
- [ ] Tests Chrome/Edge/Safari

---

### Mois 2 : Upgrade Coqui (Si succ√®s)

**Uniquement si** :
- >30% users utilisent vocal
- MRR >2000‚Ç¨ (peut absorber co√ªt GPU)
- Feedback "am√©liorer qualit√© voix"

**Actions** :
- [ ] Louer GPU RunPod on-demand
- [ ] Installer Coqui XTTS-v2
- [ ] Voice cloning pour voix Hel≈ç unique
- [ ] Tests A/B Piper vs Coqui
- [ ] Switch production

---

## üé® VOICE CLONING - Cr√©er voix unique Hel≈ç

### Avec Coqui XTTS-v2

**√âtapes** :

1. **Enregistrer √©chantillon voix** (10-15 secondes)
   - Phrase calme, apaisante
   - Bonne qualit√© audio
   - Pas de bruit de fond
   - Exemple : "Bonjour, je suis Hel≈ç. Je suis l√† pour vous accompagner avec douceur. Prenez votre temps, nous avan√ßons ensemble √† votre rythme."

2. **Sauvegarder** : `backend/models/helo_voice.wav`

3. **Utiliser** :

```python
# Dans CoquiTTS.__init__
self.speaker_wav = "/app/models/helo_voice.wav"

# Maintenant toutes les synth√®ses utilisent cette voix !
```

**R√©sultat** : Voix 100% unique √† IAcompagnon ‚ú®

---

## üîä EXEMPLES DE VOIX DISPONIBLES

### √âcouter avant de choisir

**Piper fran√ßais** :
- https://rhasspy.github.io/piper-samples/ ‚Üí Chercher "French"
- `fr_FR-siwis-medium` : Voix f√©minine douce ‚≠ê
- `fr_FR-upmc-medium` : Voix f√©minine claire

**Coqui XTTS-v2** :
- https://huggingface.co/spaces/coqui/xtts ‚Üí Tester avec texte fran√ßais
- Qualit√© excellente, tr√®s naturelle

---

## ‚úÖ CHECKLIST D√âVELOPPEUR

### Backend

- [ ] Installer Piper TTS (`pip install piper-tts`)
- [ ] T√©l√©charger mod√®le fr_FR-siwis-medium.onnx
- [ ] Cr√©er service `app/services/tts.py` (classe PiperTTS)
- [ ] Endpoint `/api/voice/synthesize` (POST)
- [ ] Installer Whisper (`pip install openai` ou `pip install whisper`)
- [ ] Endpoint `/api/voice/transcribe` (POST)
- [ ] Variables env : OPENAI_API_KEY (si Whisper API)
- [ ] Dockerfile avec Piper + mod√®les
- [ ] Tests unitaires endpoints

### Frontend

- [ ] Hook `hooks/useBackendVoice.js` (appel API TTS/STT)
- [ ] Fallback Web Speech API si backend indisponible
- [ ] Bouton micro dans Chat.jsx
- [ ] Overlay avatar mode vocal
- [ ] Lecture audio base64 ‚Üí HTMLAudioElement
- [ ] Animation avatar synchronis√©e avec audio
- [ ] Gestion erreurs (micro refus√©, API down, etc.)

---

## üí° QUESTIONS FINALES

### Q1 : Quelle voix pour Hel≈ç ?

**Options** :
1. Utiliser `fr_FR-siwis-medium` (Piper) ‚Üí Douce, f√©minine
2. Voice cloning avec Coqui ‚Üí Voix 100% unique
3. Tester plusieurs et choisir

**Ma suggestion** : Commencer avec siwis-medium, puis voice cloning si budget

### Q2 : H√©bergement GPU n√©cessaire ?

**Phase 1 (Piper)** : NON
- Fonctionne sur CPU
- Hetzner CPX31 (13.90‚Ç¨/mois) suffit

**Phase 2 (Coqui)** : OUI
- RunPod on-demand : 0.30$/heure
- Ou Hetzner CPU puissant (lent mais fonctionne)

### Q3 : Latence acceptable ?

**Piper** : 300-500ms ‚Üí Excellent ‚úÖ  
**Coqui (GPU)** : 1-2s ‚Üí Acceptable ‚úÖ  
**Coqui (CPU)** : 5-10s ‚Üí Trop lent ‚ùå  

---

## üöÄ VALIDATION

**√ätes-vous d'accord avec** :

1. ‚úÖ Phase 1 : Piper TTS (gratuit, qualit√© 7.5/10)
2. ‚úÖ STT : Whisper API (0.11‚Ç¨/user/mois)
3. ‚úÖ Phase 2 : Upgrade Coqui si succ√®s (qualit√© 9/10)
4. ‚úÖ Voice cloning pour voix unique Hel≈ç

**C'est bon pour vous ?** üéôÔ∏è
